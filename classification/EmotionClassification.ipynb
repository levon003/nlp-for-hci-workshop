{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotion Classification\n",
    "===\n",
    "\n",
    "Multilabel classification class 'E-C' in [SemEval 2018 Task 1](https://competitions.codalab.org/competitions/17751).\n",
    "\n",
    "We use SpaCy for preprocessing and Vowpal Wabbit for training and evaluating classifiers.\n",
    "\n",
    "For getting started with Vowpal Wabbit, see here: https://github.com/hal3/vwnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import matplotlib\n",
    "import pylab as pl\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/levon003/repos/nlp-for-hci-workshop')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data is stored relative to the root of the git repository\n",
    "git_root_dir = !git rev-parse --show-toplevel\n",
    "git_root_dir = Path(git_root_dir[0].strip())\n",
    "git_root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_dir = git_root_dir / 'data' / 'SemEval2018-Task1' / 'E-c'\n",
    "train = ec_dir / \"2018-E-c-En-train.txt\"\n",
    "valid = ec_dir / \"2018-E-c-En-dev.txt\"\n",
    "test = ec_dir / \"2018-E-c-En-test-gold.txt\"\n",
    "assert train.exists() and valid.exists() and test.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6838, 886, 3259)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train, sep='\\t')\n",
    "valid_df = pd.read_csv(valid, sep='\\t')\n",
    "test_df = pd.read_csv(test, sep='\\t')\n",
    "len(train_df), len(valid_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-21441</td>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-31535</td>\n",
       "      <td>Whatever you decide to do make sure it makes y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-21068</td>\n",
       "      <td>@Max_Kellerman  it also helps that the majorit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-En-31436</td>\n",
       "      <td>Accept the challenges so that you can literall...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-En-22195</td>\n",
       "      <td>My roommate: it's okay that we can't spell bec...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  anger  \\\n",
       "0  2017-En-21441  “Worry is a down payment on a problem you may ...      0   \n",
       "1  2017-En-31535  Whatever you decide to do make sure it makes y...      0   \n",
       "2  2017-En-21068  @Max_Kellerman  it also helps that the majorit...      1   \n",
       "3  2017-En-31436  Accept the challenges so that you can literall...      0   \n",
       "4  2017-En-22195  My roommate: it's okay that we can't spell bec...      1   \n",
       "\n",
       "   anticipation  disgust  fear  joy  love  optimism  pessimism  sadness  \\\n",
       "0             1        0     0    0     0         1          0        0   \n",
       "1             0        0     0    1     1         1          0        0   \n",
       "2             0        1     0    1     0         1          0        0   \n",
       "3             0        0     0    1     0         1          0        0   \n",
       "4             0        1     0    0     0         0          0        0   \n",
       "\n",
       "   surprise  trust  \n",
       "0         0      1  \n",
       "1         0      0  \n",
       "2         0      0  \n",
       "3         0      0  \n",
       "4         0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“', 'Worry', 'is', 'a', 'down', 'payment', 'on', 'a', 'problem', 'you', 'may', 'never', 'have', \"'\", '.', '\\xa0', 'Joyce', 'Meyer', '.', ' ', '#', 'motivation', '#', 'leadership', '#', 'worry']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(train_df.iloc[0].Tweet)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“ \" PUNCT `` compound “ False False\n",
      "Worry worry NOUN NN nsubj Xxxxx True False\n",
      "is be VERB VBZ ROOT xx True True\n",
      "a a DET DT det x True True\n",
      "down down ADJ JJ amod xxxx True True\n",
      "payment payment NOUN NN attr xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "a a DET DT det x True True\n",
      "problem problem NOUN NN pobj xxxx True False\n",
      "you -PRON- PRON PRP nsubj xxx True True\n",
      "may may VERB MD aux xxx True True\n",
      "never never ADV RB neg xxxx True True\n",
      "have have VERB VB relcl xxxx True True\n",
      "' ' PUNCT '' punct ' False False\n",
      ". . PUNCT . punct . False False\n",
      "     SPACE     False False\n",
      "Joyce joyce PROPN NNP compound Xxxxx True False\n",
      "Meyer meyer PROPN NNP ROOT Xxxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "    SPACE     False False\n",
      "# # NOUN NN nmod # False False\n",
      "motivation motivation NOUN NN compound xxxx True False\n",
      "# # NOUN NN compound # False False\n",
      "leadership leadership NOUN NN compound xxxx True False\n",
      "# # NOUN NN nsubj # False False\n",
      "worry worry VERB VBP ROOT xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Lana Yarosh\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " visited \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Kazakhstan\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    last week\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(nlp(\"Lana Yarosh visited Kazakhstan last week.\"), style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"246-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Lana</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">Yarosh</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">visited</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Kazakhstan</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">last</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">week.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-246-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-246-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-246-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-246-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-246-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-246-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-246-0-3\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-246-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,179.0 L762,167.0 778,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-246-0-4\" stroke-width=\"2px\" d=\"M420,177.0 C420,2.0 925.0,2.0 925.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-246-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">npadvmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M925.0,179.0 L933.0,167.0 917.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(\"Lana Yarosh visited Kazakhstan last week.\"), style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 796 ms, total: 11.5 s\n",
      "Wall time: 32.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# the default english model is relatively small... but using the larger model (850MB+) gives us word vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10983"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['test'] = False\n",
    "valid_df['test'] = False\n",
    "test_df['test'] = True\n",
    "df = pd.concat([train_df, valid_df, test_df])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10983it [11:21, 16.12it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "embedding = []\n",
    "for doc in tqdm(nlp.pipe(df.Tweet, batch_size=1000, n_threads=3)):\n",
    "    tokens.append([token.text for token in doc])\n",
    "    embedding.append(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('“', -0.15849355),\n",
       " ('Worry', 0.28258097),\n",
       " ('is', -0.061569724),\n",
       " ('a', 0.01659168),\n",
       " ('down', 0.007948801),\n",
       " ('payment', -0.039756116),\n",
       " ('on', 0.11326242),\n",
       " ('a', -0.17677411),\n",
       " ('problem', 0.012483919),\n",
       " ('you', 1.7204615),\n",
       " ('may', -0.2550403),\n",
       " ('never', -0.011027425),\n",
       " ('have', 0.08156482),\n",
       " (\"'\", 0.033338226),\n",
       " ('.', -0.20137876),\n",
       " ('\\xa0', -0.03994038),\n",
       " ('Joyce', -0.041898027),\n",
       " ('Meyer', 1.0718476),\n",
       " ('.', -0.1999955),\n",
       " (' ', 0.102360114),\n",
       " ('#', 0.014081122),\n",
       " ('motivation', 0.060670152),\n",
       " ('#', -0.059404768),\n",
       " ('leadership', 0.025720209),\n",
       " ('#', 0.08948427),\n",
       " ('worry', 0.018438809)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(tokens[0], embedding[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = tokens\n",
    "df['embedding'] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.to_pickle(git_root_dir / 'data' / 'SemEval2018-Task1' / 'E-c' / 'tokenized.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10983"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(git_root_dir / 'data' / 'SemEval2018-Task1' / 'E-c' / 'tokenized.pkl')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7724, 3259)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df[~df.test]\n",
    "test_df = df[df.test]\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "      <th>test</th>\n",
       "      <th>tokens</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-En-21441</td>\n",
       "      <td>“Worry is a down payment on a problem you may ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[“, Worry, is, a, down, payment, on, a, proble...</td>\n",
       "      <td>[-0.15849355, 0.28258097, -0.061569724, 0.0165...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-En-31535</td>\n",
       "      <td>Whatever you decide to do make sure it makes y...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[Whatever, you, decide, to, do, make, sure, it...</td>\n",
       "      <td>[-0.04233078, 0.2347169, -0.28917667, -0.03889...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-En-21068</td>\n",
       "      <td>@Max_Kellerman  it also helps that the majorit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[@Max_Kellerman,  , it, also, helps, that, the...</td>\n",
       "      <td>[-0.12489866, 0.25080636, -0.047889024, -0.047...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index             ID  \\\n",
       "0        0      0  2017-En-21441   \n",
       "1        1      1  2017-En-31535   \n",
       "2        2      2  2017-En-21068   \n",
       "\n",
       "                                               Tweet  anger  anticipation  \\\n",
       "0  “Worry is a down payment on a problem you may ...      0             1   \n",
       "1  Whatever you decide to do make sure it makes y...      0             0   \n",
       "2  @Max_Kellerman  it also helps that the majorit...      1             0   \n",
       "\n",
       "   disgust  fear  joy  love  optimism  pessimism  sadness  surprise  trust  \\\n",
       "0        0     0    0     0         1          0        0         0      1   \n",
       "1        0     0    1     1         1          0        0         0      0   \n",
       "2        1     0    1     0         1          0        0         0      0   \n",
       "\n",
       "    test                                             tokens  \\\n",
       "0  False  [“, Worry, is, a, down, payment, on, a, proble...   \n",
       "1  False  [Whatever, you, decide, to, do, make, sure, it...   \n",
       "2  False  [@Max_Kellerman,  , it, also, helps, that, the...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.15849355, 0.28258097, -0.061569724, 0.0165...  \n",
       "1  [-0.04233078, 0.2347169, -0.28917667, -0.03889...  \n",
       "2  [-0.12489866, 0.25080636, -0.047889024, -0.047...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Vowpal Wabbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION_LIST = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "# write the dataframe to a file in the vowpal wabbit format\n",
    "def to_vw_format(df, filename, emotion='anger'):\n",
    "    with open(filename, 'w') as outfile:\n",
    "        for i in range(len(df)):\n",
    "            row = df.iloc[i]\n",
    "            if emotion == 'all':\n",
    "                # we will use a cost-sensitive model\n",
    "                labels = [row[emotion] for emotion in EMOTION_LIST]\n",
    "                labels = [(label - 1) * -1 for label in labels] \n",
    "                label = \" \".join([f\"{emotion}:{int_label}\" for emotion, int_label in zip(EMOTION_LIST, labels)])\n",
    "            else: # binary classification of a single emotion\n",
    "                label = \"+1\" if row[emotion] == 1 else \"-1\"\n",
    "            features = \" \".join([token.replace(\":\", \"COLON\") for token in row.tokens])\n",
    "            line = f\"{label} {row.ID}|T {features}\\n\"\n",
    "            outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.97 s, sys: 8 ms, total: 1.98 s\n",
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_filename = ec_dir / 'vw_train_anger.txt'\n",
    "test_filename = ec_dir / 'vw_test_anger.txt'\n",
    "to_vw_format(train_df, train_filename, emotion='anger')\n",
    "to_vw_format(test_df, test_filename, emotion='anger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 2017-En-21441|T “ Worry is a down payment on a problem you may never have ' .   Joyce Meyer .   # motivation # leadership # worry\r\n",
      "-1 2017-En-31535|T Whatever you decide to do make sure it makes you # happy .\r\n",
      "+1 2017-En-21068|T @Max_Kellerman   it also helps that the majority of NFL coaching is inept . Some of Bill O'Brien 's play calling was wow , ! # GOPATS\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 {train_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/sentiment.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_train_anger.txt.cache\n",
      "Reading datafile = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_train_anger.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  -1.0000  -1.0000       26\n",
      "0.000000 0.000000            2            2.0  -1.0000  -1.0000       14\n",
      "0.250000 0.500000            4            4.0  -1.0000  -1.0000       22\n",
      "0.500000 0.750000            8            8.0   1.0000  -1.0000       17\n",
      "0.437500 0.375000           16           16.0   1.0000   1.0000        9\n",
      "0.625000 0.812500           32           32.0  -1.0000   1.0000       13\n",
      "0.531250 0.437500           64           64.0  -1.0000   1.0000       24\n",
      "0.476562 0.421875          128          128.0   1.0000   1.0000       20\n",
      "0.417969 0.359375          256          256.0  -1.0000  -1.0000       24\n",
      "0.353516 0.289062          512          512.0  -1.0000  -1.0000       27\n",
      "0.338867 0.324219         1024         1024.0  -1.0000   1.0000       19\n",
      "0.312988 0.287109         2048         2048.0  -1.0000  -1.0000       21\n",
      "0.265869 0.218750         4096         4096.0  -1.0000  -1.0000       21\n",
      "0.261826 0.261826         8192         8192.0   1.0000  -1.0000       16 h\n",
      "0.246839 0.231868        16384        16384.0  -1.0000  -1.0000       24 h\n",
      "0.243473 0.240110        32768        32768.0  -1.0000  -1.0000       32 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 6952\n",
      "passes used = 5\n",
      "weighted example sum = 34760.000000\n",
      "weighted label sum = -9120.000000\n",
      "average loss = 0.231865 h\n",
      "best constant = -0.262371\n",
      "best constant's loss = 0.931162\n",
      "total feature number = 720570\n"
     ]
    }
   ],
   "source": [
    "!vw -k -c -b 28 --passes 20 --binary {train_filename} -f {ec_dir}/sentiment.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\r\n",
      "predictions = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_test_anger.txt.pred\r\n",
      "Num weight bits = 28\r\n",
      "learning rate = 0.5\r\n",
      "initial_t = 0\r\n",
      "power_t = 0.5\r\n",
      "using no cache\r\n",
      "Reading datafile = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_test_anger.txt\r\n",
      "num sources = 1\r\n",
      "average  since         example        example  current  current  current\r\n",
      "loss     last          counter         weight    label  predict features\r\n",
      "1.000000 1.000000            1            1.0   1.0000  -1.0000       20\r\n",
      "0.500000 0.000000            2            2.0  -1.0000  -1.0000       24\r\n",
      "0.250000 0.000000            4            4.0  -1.0000  -1.0000       12\r\n",
      "0.250000 0.250000            8            8.0  -1.0000  -1.0000       16\r\n",
      "0.250000 0.250000           16           16.0  -1.0000  -1.0000       21\r\n",
      "0.187500 0.125000           32           32.0  -1.0000  -1.0000        4\r\n",
      "0.187500 0.187500           64           64.0  -1.0000  -1.0000       13\r\n",
      "0.210938 0.234375          128          128.0  -1.0000  -1.0000       18\r\n",
      "0.203125 0.195312          256          256.0  -1.0000  -1.0000        5\r\n",
      "0.199219 0.195312          512          512.0  -1.0000  -1.0000       20\r\n",
      "0.194336 0.189453         1024         1024.0   1.0000   1.0000       27\r\n",
      "0.209473 0.224609         2048         2048.0   1.0000  -1.0000       10\r\n",
      "\r\n",
      "finished run\r\n",
      "number of examples per pass = 3259\r\n",
      "passes used = 1\r\n",
      "weighted example sum = 3259.000000\r\n",
      "weighted label sum = -1057.000000\r\n",
      "average loss = 0.210187\r\n",
      "best constant = -0.324333\r\n",
      "best constant's loss = 0.894808\r\n",
      "total feature number = 67195\r\n"
     ]
    }
   ],
   "source": [
    "!vw --binary -t -i {ec_dir}/sentiment.model -p {test_filename}.pred {test_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3259, [0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds_filename = str(test_filename) + \".pred\"\n",
    "with open(test_preds_filename, 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "    preds_raw = [line.split()[0] for line in lines]\n",
    "    preds = [0 if pred == '-1' else 1 for pred in preds_raw]\n",
    "len(preds), preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/levon003/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "assert len(preds) == len(test_df)\n",
    "test_df['pred_anger'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7898128260202516"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute simple accuracy\n",
    "np.sum(test_df.pred_anger == test_df.anger) / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "anger not present       0.83      0.86      0.84      2158\n",
      "    anger present       0.70      0.65      0.68      1101\n",
      "\n",
      "        micro avg       0.79      0.79      0.79      3259\n",
      "        macro avg       0.77      0.76      0.76      3259\n",
      "     weighted avg       0.79      0.79      0.79      3259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "print(sklearn.metrics.classification_report(test_df.anger, test_df.pred_anger, \n",
    "                                            target_names=[\"anger not present\", \"anger present\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass\n",
    "\n",
    "Using a cost-sensitive one-against-all model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.78 s, sys: 12 ms, total: 2.79 s\n",
      "Wall time: 2.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_filename = ec_dir / 'vw_train_all.txt'\n",
    "test_filename = ec_dir / 'vw_test_all.txt'\n",
    "to_vw_format(train_df, train_filename, emotion='all')\n",
    "to_vw_format(test_df, test_filename, emotion='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger:1 anticipation:0 disgust:1 fear:1 joy:1 love:1 optimism:0 pessimism:1 sadness:1 surprise:1 trust:0 2017-En-21441|T “ Worry is a down payment on a problem you may never have ' .   Joyce Meyer .   # motivation # leadership # worry\r\n",
      "anger:1 anticipation:1 disgust:1 fear:1 joy:0 love:0 optimism:0 pessimism:1 sadness:1 surprise:1 trust:1 2017-En-31535|T Whatever you decide to do make sure it makes you # happy .\r\n",
      "anger:0 anticipation:1 disgust:0 fear:1 joy:0 love:1 optimism:0 pessimism:1 sadness:1 surprise:1 trust:1 2017-En-21068|T @Max_Kellerman   it also helps that the majority of NFL coaching is inept . Some of Bill O'Brien 's play calling was wow , ! # GOPATS\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 3 {train_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(EMOTION_LIST)\n",
    "named_labels = \",\".join(EMOTION_LIST)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-grams for all namespaces.\n",
      "Generating 1-skips for all namespaces.\n",
      "parsed 11 named labels\n",
      "final_regressor = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/sentiment_all.model\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_train_all.txt.cache\n",
      "Reading datafile = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_train_all.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0    known    anger       73\n",
      "1.000000 1.000000            2            2.0    known anticipation       37\n",
      "0.500000 0.000000            4            4.0    known optimism       61\n",
      "0.750000 1.000000            8            8.0    known optimism       46\n",
      "0.625000 0.500000           16           16.0    known  disgust       22\n",
      "0.593750 0.562500           32           32.0    known  disgust       34\n",
      "0.546875 0.500000           64           64.0    known      joy       67\n",
      "0.507812 0.468750          128          128.0    known  disgust       55\n",
      "0.519531 0.531250          256          256.0    known optimism       67\n",
      "0.490234 0.460938          512          512.0    known  sadness       76\n",
      "0.443359 0.396484         1024         1024.0    known    anger       52\n",
      "0.397949 0.352539         2048         2048.0    known  sadness       58\n",
      "0.344971 0.291992         4096         4096.0    known      joy       58\n",
      "0.324532 0.324532         8192         8192.0    known  disgust       43 h\n",
      "0.301264 0.278022        16384        16384.0    known     fear       67 h\n",
      "0.285793 0.270330        32768        32768.0    known  sadness       91 h\n",
      "0.278686 0.271578        65536        65536.0    known    anger       70 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 6952\n",
      "passes used = 10\n",
      "weighted example sum = 69520.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.265544 h\n",
      "total feature number = 3975800\n",
      "Generating 2-grams for all namespaces.\n",
      "Generating 1-skips for all namespaces.\n",
      "only testing\n",
      "parsed 11 named labels\n",
      "predictions = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_test_all.txt.pred\n",
      "raw predictions = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_test_all.txt.pred.raw\n",
      "Num weight bits = 28\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /home/levon003/repos/nlp-for-hci-workshop/data/SemEval2018-Task1/E-c/vw_test_all.txt\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0    known optimism       55\n",
      "0.500000 1.000000            2            2.0    known optimism       67\n",
      "0.250000 0.000000            4            4.0    known      joy       31\n",
      "0.250000 0.250000            8            8.0    known  disgust       43\n",
      "0.187500 0.125000           16           16.0    known      joy       58\n",
      "0.187500 0.187500           32           32.0    known      joy        7\n",
      "0.234375 0.281250           64           64.0    known      joy       34\n",
      "0.265625 0.296875          128          128.0    known      joy       49\n",
      "0.250000 0.234375          256          256.0    known    anger       10\n",
      "0.267578 0.285156          512          512.0    known  disgust       55\n",
      "0.263672 0.259766         1024         1024.0    known  sadness       76\n",
      "0.265137 0.266602         2048         2048.0    known  disgust       25\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 3259\n",
      "passes used = 1\n",
      "weighted example sum = 3259.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.260203\n",
      "total feature number = 185280\n"
     ]
    }
   ],
   "source": [
    "!vw -k -c -b 28 --csoaa {num_classes} -d {train_filename} -f {ec_dir}/sentiment_all.model --passes 20 --named_labels {named_labels} --ngram 2 --skips 1\n",
    "!vw -t -i {ec_dir}/sentiment_all.model -d {test_filename} -p {test_filename}.pred -r {test_filename}.pred.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimism 2018-En-01559\n",
      "optimism 2018-En-03739\n",
      "anger 2018-En-00385\n",
      "\n",
      "1:0.829435 2:0.675799 3:0.679984 4:0.710117 5:0.832285 6:0.964882 7:0.62584 8:0.941957 9:0.772772 10:1.12944 11:0.896213 2018-En-01559\n",
      "1:0.789542 2:0.548801 3:0.729407 4:0.884375 5:0.505027 6:0.871636 7:0.485595 8:0.803483 9:0.612237 10:1.00483 11:0.870229 2018-En-03739\n",
      "1:0.372602 2:0.664358 3:0.55658 4:0.616096 5:0.502329 6:0.769712 7:0.644689 8:0.571755 9:0.54561 10:0.772304 11:0.808059 2018-En-00385\n"
     ]
    }
   ],
   "source": [
    "test_preds_filename = str(test_filename) + \".pred\"\n",
    "test_raw_preds_filename = str(test_filename) + \".pred.raw\"\n",
    "!head -n 3 {test_preds_filename}\n",
    "!echo\n",
    "!head -n 3 {test_raw_preds_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3259"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(test_raw_preds_filename, 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "predictions = []\n",
    "for line in lines:\n",
    "    if line.strip() == \"\":\n",
    "        continue\n",
    "    tokens = line.split()[:-1]\n",
    "    assert len(tokens) == num_classes\n",
    "    prediction = {}\n",
    "    for i, token in enumerate(tokens):\n",
    "        raw_pred = float(token.split(\":\")[1])\n",
    "        prediction[EMOTION_LIST[i]] = raw_pred\n",
    "    predictions.append(prediction)\n",
    "assert len(predictions) == len(test_df)\n",
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>love</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pessimism</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>trust</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.829435</td>\n",
       "      <td>0.675799</td>\n",
       "      <td>0.679984</td>\n",
       "      <td>0.710117</td>\n",
       "      <td>0.832285</td>\n",
       "      <td>0.964882</td>\n",
       "      <td>0.625840</td>\n",
       "      <td>0.941957</td>\n",
       "      <td>0.772772</td>\n",
       "      <td>1.12944</td>\n",
       "      <td>0.896213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.789542</td>\n",
       "      <td>0.548801</td>\n",
       "      <td>0.729407</td>\n",
       "      <td>0.884375</td>\n",
       "      <td>0.505027</td>\n",
       "      <td>0.871636</td>\n",
       "      <td>0.485595</td>\n",
       "      <td>0.803483</td>\n",
       "      <td>0.612237</td>\n",
       "      <td>1.00483</td>\n",
       "      <td>0.870229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      anger  anticipation   disgust      fear       joy      love  optimism  \\\n",
       "0  0.829435      0.675799  0.679984  0.710117  0.832285  0.964882  0.625840   \n",
       "1  0.789542      0.548801  0.729407  0.884375  0.505027  0.871636  0.485595   \n",
       "\n",
       "   pessimism   sadness  surprise     trust  \n",
       "0   0.941957  0.772772   1.12944  0.896213  \n",
       "1   0.803483  0.612237   1.00483  0.870229  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = pd.DataFrame(predictions)\n",
    "test_preds.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3259, 11)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify the ground truth\n",
    "n_samples = len(test_df)\n",
    "n_classes = num_classes\n",
    "y_true = test_df.loc[:, EMOTION_LIST].values\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3259, 11) (3259, 11) (3259, 11)\n",
      "0.7416444989753931 0.7427934072169889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3259, 11)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_score_raw = test_preds.loc[:, EMOTION_LIST].values\n",
    "y_score_1 = 1 - np.clip(y_score_raw, 0, 1)\n",
    "y_score_2 = 1 - ((y_score_raw - y_score_raw.min(0)) / y_score_raw.ptp(0))\n",
    "print(y_score_raw.shape, y_score_1.shape, y_score_2.shape)\n",
    "assert y_score_1.shape == y_score_2.shape\n",
    "y_score = y_score_1\n",
    "assert np.isfinite(y_score).all()\n",
    "print(sklearn.metrics.roc_auc_score(y_true, y_score_1), sklearn.metrics.roc_auc_score(y_true, y_score_2))\n",
    "y_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion         # Predicted Present\n",
      "========================================\n",
      "anger            946\n",
      "anticipation     124\n",
      "disgust         1090\n",
      "fear             342\n",
      "joy             1175\n",
      "love             192\n",
      "optimism         794\n",
      "pessimism        108\n",
      "sadness          681\n",
      "surprise          19\n",
      "trust             20\n"
     ]
    }
   ],
   "source": [
    "y_pred = (y_score > 0.5).astype(int)\n",
    "print(f\"{'Emotion':15} # Predicted Present\")\n",
    "print(\"=\"*40)\n",
    "for i, total_predicted in enumerate(np.sum(y_pred, axis=0)):\n",
    "    print(f\"{EMOTION_LIST[i]:15} {total_predicted:4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.69      0.59      0.64      1101\n",
      "anticipation       0.28      0.08      0.13       425\n",
      "     disgust       0.60      0.59      0.60      1099\n",
      "        fear       0.65      0.46      0.54       485\n",
      "         joy       0.81      0.66      0.73      1442\n",
      "        love       0.70      0.26      0.38       516\n",
      "    optimism       0.67      0.47      0.55      1143\n",
      "   pessimism       0.41      0.12      0.18       375\n",
      "     sadness       0.67      0.47      0.55       960\n",
      "    surprise       0.42      0.05      0.08       170\n",
      "       trust       0.25      0.03      0.06       153\n",
      "\n",
      "   micro avg       0.67      0.47      0.55      7869\n",
      "   macro avg       0.56      0.34      0.40      7869\n",
      "weighted avg       0.64      0.47      0.53      7869\n",
      " samples avg       0.60      0.49      0.51      7869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(y_true, y_pred, target_names=EMOTION_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17213869285056765"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Strict accuracy\n",
    "sklearn.metrics.accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like a pretty hard task!\n",
    "\n",
    "The winning team (from the National Technical University of Athens) achieved accuracy, micro F1, and macro F1 of 0.588, 0.701, and 0.528 respectively.  We achieved 0.17, 0.55, and 0.4!\n",
    "\n",
    "See also [all results](https://docs.google.com/spreadsheets/d/1yzyBC7uf4Di38QooN7QbUGqsokK51FShUXCfUJFZmLs/edit#gid=1360997704) for this task.\n",
    "\n",
    "The winning team also made their code available: https://github.com/cbaziotis/ntua-slp-semeval2018\n",
    "\n",
    "Their approach is described in [this](https://arxiv.org/pdf/1804.06658.pdf) technical paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
